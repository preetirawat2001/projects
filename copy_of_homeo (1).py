# -*- coding: utf-8 -*-
"""Copy of homeo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/126_SgNXxRcyOhFlErg-e3GNfOSd6eiw3
"""

!pip install pymupdf
!pip install fuzzywuzzy[speedup]

import fitz
import json
import re

def clean_line(line):
    junk_phrases = [
        "Â© Copyright 2000, Archibel S.A.",
        "Encyclopaedia Homeopathica"
    ]
    return not any(junk in line for junk in junk_phrases)


def pdf_to_cleaned_json(pdf_path, output_path, start_page=3, indices_to_remove=None):
    doc = fitz.open(pdf_path)
    remedies = []
    current_medicine = None
    current_section = None

    for page_num in range(start_page - 1, len(doc)):
        text = doc[page_num].get_text()
        lines = text.split('\n')

        for line in lines:
            line = line.strip()

            if not clean_line(line):
                continue


            if re.match(r'^[A-Z][a-z]+\s+[a-z]+$', line):
                if current_medicine:
                    remedies.append(current_medicine)

                current_medicine = {
                    "medicine": line,
                    "symptoms": {}
                }
                current_section = None


            elif re.match(r'^[A-Z][a-z]+$', line) and len(line.split()) == 1:
                current_section = line
                if current_medicine:
                    current_medicine["symptoms"][current_section] = []


            elif current_medicine and current_section:
                current_medicine["symptoms"][current_section].append(line)


    if current_medicine:
        remedies.append(current_medicine)

    remedies = [entry for entry in remedies if entry['medicine'].strip().lower() != "materia medica"]

    if indices_to_remove:
        remedies = [entry for i, entry in enumerate(remedies) if i not in indices_to_remove]


    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(remedies, f, ensure_ascii=False, indent=2)

    print(f"Final cleaned JSON saved to {output_path}")


pdf_to_cleaned_json(
    pdf_path="/content/pathak.pdf",
    output_path="/content/phatak_final.json",
    start_page=3,
    indices_to_remove=[1, 2, 3, 4]
)

pip install wordcloud matplotlib

from wordcloud import WordCloud
import matplotlib.pyplot as plt


json_path = "/content/phatak_final.json"
with open(json_path, "r", encoding="utf-8") as f:
    data = json.load(f)


all_text = ""
for entry in data:
    symptoms = entry.get("symptoms", {})
    for section, lines in symptoms.items():
        all_text += " ".join(lines) + " "


wordcloud = WordCloud(
    width=1600,
    height=800,
    background_color="white",
    colormap="tab10"
).generate(all_text)


plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout()
plt.show()

medicine_names = []
symptom_counts = {}
all_sections = set()


for entry in data:
    med_name = entry.get("medicine", "Unknown")
    medicine_names.append(med_name)

    symptoms = entry.get("symptoms", {})
    count = sum(len(v) for v in symptoms.values())
    symptom_counts[med_name] = count

    all_sections.update(symptoms.keys())


print("Total Medicines:", len(medicine_names))
print("Medicine Names:", medicine_names[:10], "...")  # First 10
print("Unique Symptom Categories:", sorted(all_sections))
print("\nSymptom Count per Medicine (sample):")
for med in list(symptom_counts.keys())[:5]:  # Show 5 samples
    print(f"  - {med}: {symptom_counts[med]} symptoms")

import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame(flat_data)
symptom_counts = df['medicine'].value_counts().head(10)

symptom_counts.plot(kind='bar', figsize=(10, 5), color='skyblue')
plt.title("Top 10 Medicines by Number of Symptoms")
plt.xlabel("Medicine")
plt.ylabel("Symptom Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from collections import Counter

section_counts = Counter()
for entry in data:
    section_counts.update(entry['symptoms'].keys())

pd.Series(section_counts).sort_values(ascending=False).head(10).plot(kind='barh', color='salmon')
plt.title("Most Common Symptom Categories")
plt.xlabel("Count")
plt.tight_layout()
plt.show()

!pip install -q openai langchain sentence-transformers faiss-cpu

data[0]

def extract_symptom_text(remedy_entry):
    med_name = remedy_entry['medicine']
    all_symptoms = []
    for section, symptoms in remedy_entry.get('symptoms', {}).items():
        for symptom in symptoms:
            if symptom.strip():
                all_symptoms.append(f"{section}: {symptom.strip()}")
    return med_name, all_symptoms

flat_data = []
for remedy in data:
    med, symptoms = extract_symptom_text(remedy)
    for symptom in symptoms:
        flat_data.append({
            "medicine": med,
            "symptom": symptom
        })


flat_data[:3]

!pip install faiss-cpu

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np


model = SentenceTransformer('all-MiniLM-L6-v2')


texts = [entry["symptom"] for entry in flat_data]
embeddings = model.encode(texts, show_progress_bar=True)


index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))


symptom_to_medicine = {i: flat_data[i] for i in range(len(flat_data))}

np.save("embeddings.npy", embeddings)

import numpy as np

embeddings = np.load("embeddings.npy")
print(embeddings.shape)

faiss.write_index(index, "faiss_index.idx")

with open("flat_data.json", "w", encoding="utf-8") as f:
    json.dump(flat_data, f, ensure_ascii=False, indent=2)

print("Saved embeddings.npy, faiss_index.idx, and flat_data.json")

for i in range(5):
    print(f"Symptom: {flat_data[i]['symptom']}")
    print(f"Embedding: {embeddings[i][:5]}...")
    print()

def search_symptom(query, k=5):
    query_embedding = model.encode([query])
    D, I = index.search(np.array(query_embedding), k)
    results = []
    for i in I[0]:
        entry = symptom_to_medicine[i]
        results.append(f"{entry['medicine']}: {entry['symptom']}")
    return results

from transformers import pipeline

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
generator = pipeline("text-generation", model=model_name)

def build_context(query, k=5):
    results = search_symptom(query, k)
    return "\n".join(results)

def build_prompt(query, context):
    return f"""You are a helpful homeopathy assistant.
Use only the information provided below to answer the question.

Symptoms and remedies:
{context}

Question: {query}

Answer only based on the above symptoms and remedies. Do not add any extra information or speculation.
"""

def generate_answer(query, k=5):
    context = build_context(query, k)
    prompt = build_prompt(query, context)
    response = generator(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)

    # Clean the output to remove everything before "Answer:"
    raw_text = response[0]['generated_text']
    answer_start = raw_text.find("Answer:")
    if answer_start != -1:
        cleaned_answer = raw_text[answer_start + len("Answer:"):].strip()
    else:
        cleaned_answer = raw_text.strip()

    return cleaned_answer

user_query = "What remedies are suggested for headaches with nausea?"
answer = generate_answer(user_query)
print(answer)

query = "feeling headache, vomitting?"
answer = generate_answer(query)
print("ðŸ§  Answer:\n", answer)

"""# GUI"""

!pip install -q gradio sentence-transformers faiss-cpu transformers

import gradio as gr
import numpy as np
import faiss
import json
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# Load your models and data
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = np.load("embeddings.npy")
index = faiss.read_index("faiss_index.idx")
with open("flat_data.json", "r", encoding="utf-8") as f:
    flat_data = json.load(f)

symptom_to_medicine = {i: flat_data[i] for i in range(len(flat_data))}
generator = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")

def search_symptom(query, k=5):
    query_embedding = model.encode([query])
    D, I = index.search(np.array(query_embedding), k)
    results = []
    for i in I[0]:
        entry = symptom_to_medicine[i]
        results.append(f"{entry['medicine']}: {entry['symptom']}")
    return results

def build_context(query, k=5):
    results = search_symptom(query, k)
    return "\n".join(results)

def build_prompt(query, context):
    return f"""Helpful homeopathy assistant.

Symptoms and remedies:
{context}

Question: {query}
Answer only based on the above symptoms and remedies. Do not add any extra information or speculation.

"""


def generate_answer(query, k=5):
    context = build_context(query, k)
    prompt = build_prompt(query, context)
    response = generator(prompt, max_new_tokens=150, do_sample=False)
    raw_text = response[0]['generated_text']
    answer_start = raw_text.find("Answer:")
    if answer_start != -1:
        cleaned_answer = raw_text[answer_start + len("Answer:"):].strip()
    else:
        cleaned_answer = raw_text.strip()


    return cleaned_answer

def gradio_interface(query):
    return generate_answer(query)

# Setting up the Gradio interface
iface = gr.Interface(
    fn=gradio_interface,
    inputs=gr.Textbox(label="Enter Symptoms", placeholder="e.g., headache, nausea"),
    outputs=gr.Textbox(label="Homeopathic Remedies"),
    title="Homeopathy Recommendation System",
    description="""
<div style="background-color: #f0f8ff; padding: 10px; border-radius: 8px; font-size: 18px; font-weight: bold;">
ðŸ©º Enter your symptoms, and get suggestions for homeopathic remedies ðŸ’ŠðŸŒ¿
</div>
"""


)

# Launching the app
iface.launch(share=True)

"""VALIDATION CHECK"""

!pip install rouge-score

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import fuzz
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer
import numpy as np

def validate_remedies(query, top_remedies, full_data, sentence_model):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)

    print(f"\nValidation for query: '{query}'\n")

    for i, remedy_name in enumerate(top_remedies):
        remedy = next((item for item in full_data if item["medicine"] == remedy_name), None)
        if not remedy:
            print(f"{i+1}. {remedy_name} not found in data.\n")
            continue

        all_symptoms = []
        for section, lines in remedy['symptoms'].items():
            all_symptoms.extend(lines)
        remedy_text = " ".join(all_symptoms)

        query_emb = sentence_model.encode([query])[0]
        remedy_emb = sentence_model.encode([remedy_text])[0]
        sentence_sim = np.dot(query_emb, remedy_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(remedy_emb))

        fuzzy_score = fuzz.token_set_ratio(query, remedy_text) / 100

        vectorizer = TfidfVectorizer().fit([query, remedy_text])
        tfidf_matrix = vectorizer.transform([query, remedy_text])
        tfidf_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]

        rouge_score = scorer.score(query, remedy_text)['rougeL'].fmeasure

        avg_score = (sentence_sim + fuzzy_score + tfidf_score + rouge_score) / 4

        print(f"{i+1}. {remedy_name}")
        print(f"   - SentenceTransformer: {sentence_sim:.4f}")
        print(f"   - FuzzyWuzzy:          {fuzzy_score:.4f}")
        print(f"   - TF-IDF:              {tfidf_score:.4f}")
        print(f"   - ROUGE-L:             {rouge_score:.4f}")
        print(f"   => Avg Score:          {avg_score:.4f}\n")

from sentence_transformers import SentenceTransformer

# Load model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Sample usage
user_query = "headache with nausea"
top_medicines = ["Gossypium herbaceum", "Borax veneta", "Indigo tinctoria","Phytolacca decandra","Lachesis mutus"]
validate_remedies(user_query, top_medicines, data, model)

